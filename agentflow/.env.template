######################################################################
#  WARNING
#  ------------------------------------------------------------------
#  This is a template for a `.env` file.  
#  • **Do NOT commit or push a populated version of this file (with real
#    secrets) to any public repository or shared location.**  
#  • Replace the placeholder values (`<your-...>` or `<...>`) with your
#    actual credentials in a private copy only.  
######################################################################

# The content of the .env file

# Used for LLM-powered modules and tools
OPENAI_API_KEY=<your-api-key-here> # If you want to use OpenAI LLM
DASHSCOPE_API_KEY=<your-api-key-here> 
TOGETHER_API_KEY=<your-api-key-here>       # if dashscope doesn't support, use TogetherAI LLM or serve the qwen2.5-7B-instruct model with vLLM.

# Used for the Google Search tool
GOOGLE_API_KEY=<your-api-key-here>

# === Optional Customized API ===
# === LLM Provider API Keys ===
ANTHROPIC_API_KEY=<your-api-key-here>      # Anthropic LLM
DEEPSEEK_API_KEY=<your-api-key-here>       # DeepSeek LLM
XAI_API_KEY=<your-api-key-here>            # Grok (xAI) LLM

# === Azure OpenAI Configuration ===
AZURE_OPENAI_API_KEY=<your-api-key-here>   # Azure OpenAI API Key
AZURE_OPENAI_ENDPOINT=https://<your-resource-name>.openai.azure.com/  # Azure OpenAI Endpoint
AZURE_OPENAI_API_VERSION=<your-api-version-here>                      # Azure OpenAI API Version
AZURE_OPENAI_DEPLOYMENT=<your-deployment-name-here>                  # Azure OpenAI Deployment Name

# === LM Studio (local OpenAI-compatible proxy) ===
# LM Studio exposes an OpenAI-compatible API server (default http://localhost:1234/v1).
# Many setups do not require a real API key; the placeholder will be accepted.
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_API_KEY=lm-studio

# === LM Studio (local OpenAI-compatible proxy) ===
# If you run LM Studio's local server, it exposes an OpenAI-compatible API.
# See: https://lmstudio.ai/docs/local-server
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_API_KEY=lm-studio   # Often ignored by LM Studio; kept for compatibility

# === LM Studio (local OpenAI-compatible proxy) ===
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_API_KEY=lm-studio  # Optional; LM Studio often ignores this
