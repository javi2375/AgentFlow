PLEASE CHANGE THE ARXIV, REPO, AND OTEHR URL in `agentflow/pyproject.toml`

# AgentFlow: In-The-Flow Agentic System Optimization for Effective Planning and Tool Use.

## Setup
### Installation
```bash
bash setup.sh
source .venv/bin/activate
# (Optional) Install `parallel` for running benchmark experiments in parallel:
sudo apt-get update
sudo apt-get install parallel
```

### Setup Environment Variables
Duplicate the `.env.template` file and rename it to `.env`.  
Next, update the variables (`OPENAI_API_KEY`, `GOOGLE_API_KEY`, `GOOGLE_CX`, `DASHSCOPE_API_KEY`) with your own keys.  
```
cp .env_template .env
```

## Quick Start
### Dataset Preparation
```bash
# train data
python data/get_train_data.py
# validation data
python data/aime24_data.py
```

After that, data dir should be:
```
data/
â”œâ”€â”€ train/
â”‚   â””â”€â”€ combined_train.parquet (182,190 samples)
â”œâ”€â”€ val/
â”‚   â””â”€â”€ aime24.parquet (30 samples)
â”œâ”€â”€ aime24_data.py
â””â”€â”€ get_train_data.py
```
### Train
Start agentflow training with tmux:
```bash
# Create tmux session and start agentflow service (Window 0)
tmux new-session -s agentflow
bash train/serve_with_logs.sh

# Create new window (Ctrl+B then C) and start training (Window 1)
bash train/train_with_logs.sh
```
**Configuration:**
All training hyperparameters are in `train/config.yaml` (model settings, tools, RL parameters, resources, etc.)

### Infer
To run inference on benchmark tasks, first ensure your planner model is being served via VLLM, then execute:
```bash
cd test
bash exp/run_all_models_all_datasets.sh
```
---










**Serving models with VLLM:**

An easy VLLM serving script can be found in `scripts/serve_vllm.sh`. This script automatically launches multiple models in parallel using tmux:

```bash
bash scripts/serve_vllm.sh
```

Before running, configure the script:
- **models**: List of model paths to serve
- **gpu_groups**: GPU allocation for each model (e.g., `"0,1"` for 2 GPUs)
- **start_port**: Starting port number (default: 8000)

The script will create a tmux session and serve each model on consecutive ports (8000, 8001, etc.) with automatic tensor parallelism based on GPU count. 

**Configuration:**
Before running, configure the script in `test/exp/run_all_models_all_datasets.sh`:
- **TASKS**: Enable/disable tasks by commenting/uncommenting (e.g., `"aime24"`, `"gameof24"`, `"bamboogle"`)
- **MODELS**: Define models with their tool configurations:
  ```bash
  MODELS=(
      "8000:vllm-IPF/AgentFlow-3B,AgentFlow-3B,Base_Generator_Tool|Python_Coder_Tool,dashscope|dashscope"
      "8001:vllm-IPF/AgentFlow-7B,AgentFlow-7B,Base_Generator_Tool|Python_Coder_Tool,dashscope|dashscope"
  )
  ```
  Format: `"port:model_path,label,tools(|-separated),engines(|-separated)"`
- **THREADS**: Number of parallel workers (default: 20)

**Results location:**
After completion, results will be organized as follows:
```
test/
â””â”€â”€ {TASK_NAME}/           # e.g., aime24, gameof24, bamboogle
    â”œâ”€â”€ logs/
    â”‚   â””â”€â”€ {MODEL_LABEL}/  # e.g., AgentFlow-7B
    â”‚       â”œâ”€â”€ 0.log       # Individual problem logs
    â”‚       â”œâ”€â”€ 1.log
    â”‚       â””â”€â”€ ...
    â”œâ”€â”€ results/
    â”‚   â””â”€â”€ {MODEL_LABEL}/
    â”‚       â”œâ”€â”€ finalresults_direct_output.json   # Detailed results with analysis
    â”‚       â”œâ”€â”€ final_scores_direct_output.json   # Final scores and statistics
    â”‚       â”œâ”€â”€ finalscore_direct_output.log      # Scoring process log
    â”‚       â”œâ”€â”€ output_0.json              # Individual problem outputs
    â”‚       â”œâ”€â”€ output_1.json
    â”‚       â””â”€â”€ ...
    â””â”€â”€ cache/              # Cached intermediate results
```

**Key result files:**
- `final_scores_direct_output.json`: Contains accuracy, correct count, wrong PIDs, and tool usage statistics
- `finalresults_direct_output.json`: Detailed results with per-problem analysis and verification
- Individual `output_{i}.json`: Full output including query, response, memory, and execution traces

## Training Logs and Outputs

### Training Logs
During training, logs are automatically saved with IP-based organization:
```
task_logs/
â””â”€â”€ {PUBLIC_IP}/
    â””â”€â”€ train_log/
        â”œâ”€â”€ training_output_0000  # First 1MB of logs
        â”œâ”€â”€ training_output_0001  # Next 1MB
        â”œâ”€â”€ training_output_0002
        â””â”€â”€ ...
```
- Logs are split into 1MB files for easier management (configurable in `train/train_with_logs.sh`)
- Maximum 5000 log files retained
- Monitor latest logs: `tail -f task_logs/{YOUR_IP}/train_log/training_output_*`

### Model Checkpoints
Trained model checkpoints are saved periodically:
```
checkpoints/
â””â”€â”€ {PROJECT_NAME}/           # e.g., AgentFlow_general (from config.yaml)
    â””â”€â”€ {EXPERIMENT_NAME}/    # e.g., rollout_all_7B_useklloss (from config.yaml)
        â”œâ”€â”€ global_step_2/
        â”‚   â”œâ”€â”€ actor/
        â”‚   â”‚   â””â”€â”€ huggingface/  # HuggingFace format (ready for inference)
        â”‚   â””â”€â”€ data.pt           # Training state
        â”œâ”€â”€ global_step_4/
        â”œâ”€â”€ global_step_6/
        â””â”€â”€ latest_checkpointed_iteration.txt  # Points to latest checkpoint
```
**Checkpoint settings** (in `train/config.yaml`):
- `trainer.save_freq`: Checkpoint frequency (default: every 2 epochs)
- `trainer.test_freq`: Validation frequency (default: every 2 epochs)
- `trainer.total_epochs`: Total training epochs (default: 5)

### Rollout Data
During training, rollout trajectories are saved for analysis(start from 0 for each restart, the actual step may be different):
```
rollout_data/
â””â”€â”€ {PUBLIC_IP}/
    â””â”€â”€ {EXPERIMENT_NAME}_{TIMESTAMP}/     # e.g., rollout_all_7B_{time_stamp}
        â”œâ”€â”€ .init.lock
        â”œâ”€â”€ .run_info
        â””â”€â”€ {MODEL_NAME}_{TIMESTAMP}/      # e.g., Qwen2.5-7B-Instruct_{time_stamp}
            â”œâ”€â”€ train/                      # Training rollouts (usually empty to save space)
            â””â”€â”€ validation/
                â”œâ”€â”€ .val.lock
                â””â”€â”€ step_0/                 # Validation at global step 0
                    â”œâ”€â”€ idx_0/              # Individual validation samples
                    â”‚   â””â”€â”€ rollout_{uuid}.json
                    â”œâ”€â”€ idx_1/
                    â””â”€â”€ ...
```

**Rollout JSON structure** (each `rollout_{uuid}.json`):
- `prompt`: Original problem/query
- `groundtruth`: Expected answer
- `answer_extracted`: Model's extracted answer
- `reward`: Reward score (0.0 for incorrect, positive for correct)
- `total_result`: Complete execution trace including:
  - `query_analysis`: Problem analysis
  - `memory`: Step-by-step tool execution history
  - `direct_output`: Final model response
  - Tool prompts and responses for each step
- `timestamp`: Rollout generation time

**Using saved checkpoints:**
The models in `checkpoints/{PROJECT}/{EXPERIMENT}/global_step_X/actor/huggingface/` can be used for:
1. **Inference via VLLM**: Configure model paths in `scripts/serve_vllm.sh`
2. **Direct loading**: Standard HuggingFace Transformers `from_pretrained()`














## Test your env before going on

vplease run the following command to test all tools:

```bash
cd agentflow/agentflow
bash ./tools/test_all_tools.sh
```

A `test.log` will be saved in each tool's file. 

Success example: 
```text
Testing all tools
Tools:
  - base_generator
  - google_search
  - python_coder
  - web_search
  - wikipedia_search

Running tests in parallel...
Testing base_generator...
âœ… base_generator passed
Testing google_search...
âœ… google_search passed
Testing python_coder...
âœ… python_coder passed
Testing wikipedia_search...
âœ… wikipedia_search passed
Testing web_search...
âœ… web_search passed

âœ… All tests passed
```

### IP test
test your public IP(just for saving the logs files)
```bash
python util/get_pub_ip.py
```

### LLM engine test
Please run the following command to test all LLM engines:

```bash
python agentflow/scripts/test_llm_engine.py
```

Example output:
```text
ğŸš€ Starting fault-tolerant test for 11 engines...
ğŸ§ª Testing: 'gpt-4o' | kwargs={}
âœ… Success: Created ChatOpenAI
ğŸ§ª Testing: 'dashscope-qwen2.5-3b-instruct' | kwargs={}
âœ… Success: Created ChatDashScope
ğŸ§ª Testing: 'gemini-1.5-pro' | kwargs={}
âœ… Success: Created ChatGemini
============================================================
ğŸ“‹ TEST SUMMARY
============================================================
âœ… Passed: 3
   â€¢ gpt-4o â†’ ChatOpenAI
   â€¢ dashscope-qwen2.5-3b-instruct â†’ ChatDashScope
   â€¢ gemini-1.5-pro â†’ ChatGemini
âŒ Failed: 8
   â€¢ azure-gpt-4 â†’ ğŸš« API key not found in environment
   â€¢ claude-3-5-sonnet â†’ ğŸš« API key not found in environment
   â€¢ deepseek-chat â†’ ğŸš« API key not found in environment
   â€¢ grok â†’ ğŸš« API key not found in environment
   â€¢ vllm-meta-llama/Llama-3-8b-instruct â†’ ğŸš« Connection failed
   â€¢ together-meta-llama/Llama-3-70b-chat-hf â†’ ğŸš« API key not found
   â€¢ ollama-llama3 â†’ ğŸš« Connection failed
   â€¢ unknown-model-123 â†’ ğŸ’¥ Unexpected error
============================================================
ğŸ‰ Testing complete. Script did NOT crash despite errors.
```

## Quick Start
